{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPACY and NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp= spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"Hi, my name is govind. I love pav bhaji.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, my name is govind.\n",
      "I love pav bhaji.\n"
     ]
    }
   ],
   "source": [
    "for sentence in doc.sents :\n",
    "    print(sentence)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi\n",
      ",\n",
      "my\n",
      "name\n",
      "is\n",
      "govind\n",
      ".\n",
      "I\n",
      "love\n",
      "pav\n",
      "bhaji\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for sentence in doc.sents:\n",
    "    for word in sentence:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91876\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi.', 'bye']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize('hi. bye')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi, my name is govind.', 'I love pav bhaji.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(\"Hi, my name is govind. I love pav bhaji.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " ',',\n",
       " 'my',\n",
       " 'name',\n",
       " 'is',\n",
       " 'govind',\n",
       " '.',\n",
       " 'I',\n",
       " 'love',\n",
       " 'pav',\n",
       " 'bhaji',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(\"Hi, my name is govind. I love pav bhaji.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tony ==> Index: 0 is_num: False is_alpha: True is_punct: False is_currency: False\n",
      "gave ==> Index: 1 is_num: False is_alpha: True is_punct: False is_currency: False\n",
      "two ==> Index: 2 is_num: True is_alpha: True is_punct: False is_currency: False\n",
      "$ ==> Index: 3 is_num: False is_alpha: False is_punct: False is_currency: True\n",
      "to ==> Index: 4 is_num: False is_alpha: True is_punct: False is_currency: False\n",
      "Peter ==> Index: 5 is_num: False is_alpha: True is_punct: False is_currency: False\n",
      ". ==> Index: 6 is_num: False is_alpha: False is_punct: True is_currency: False\n"
     ]
    }
   ],
   "source": [
    "doc2=nlp('Tony gave two $ to Peter.')\n",
    "\n",
    "for token in doc2:\n",
    "    print(token , '==>','Index:',token.i,'is_num:',token.like_num,'is_alpha:',token.is_alpha,'is_punct:',token.is_punct,'is_currency:',token.is_currency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "भैया ==> Index: 0 is_num: False is_punct: False is_currency: False\n",
      "जी ==> Index: 1 is_num: False is_punct: False is_currency: False\n",
      "! ==> Index: 2 is_num: False is_punct: True is_currency: False\n",
      "5000 ==> Index: 3 is_num: True is_punct: False is_currency: False\n",
      "₹ ==> Index: 4 is_num: False is_punct: False is_currency: True\n",
      "उधार ==> Index: 5 is_num: False is_punct: False is_currency: False\n",
      "थे ==> Index: 6 is_num: False is_punct: False is_currency: False\n",
      "वो ==> Index: 7 is_num: False is_punct: False is_currency: False\n",
      "वापस ==> Index: 8 is_num: False is_punct: False is_currency: False\n",
      "देदो ==> Index: 9 is_num: False is_punct: False is_currency: False\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank('hi')\n",
    "doc3 = nlp('भैया जी! 5000 ₹ उधार थे वो वापस देदो')\n",
    "\n",
    "for token in doc3:\n",
    "    print(token , '==>','Index:',token.i,'is_num:',token.like_num,'is_punct:',token.is_punct,'is_currency:',token.is_currency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp =spacy.blank('en')\n",
    "doc4 = nlp('''\n",
    "Look for data to help you address the question. Governments are good\n",
    "sources because data from public research is often freely available. Good\n",
    "places to start include http://www.data.gov/, and http://www.science.\n",
    "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
    "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/, \n",
    "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.data.gov/\n",
      "http://www.science\n",
      "http://data.gov.uk/.\n",
      "http://www3.norc.org/gss+website/\n",
      "http://www.europeansocialsurvey.org/.\n"
     ]
    }
   ],
   "source": [
    "for token in doc4:\n",
    "    if token.like_url==True:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two $\n",
      "500 €\n"
     ]
    }
   ],
   "source": [
    "transactions = nlp(\"Tony gave two $ to Peter, Bruce gave 500 € to Steve\")\n",
    "\n",
    "for token in transactions:\n",
    "    if token.is_currency==True:\n",
    "        print(transactions[token.i-1],transactions[token.i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nlp('''Ravi and Raju are the best friends from school days.They wanted to go for a world tour and \n",
    "visit famous cities like Paris, London, Dubai, Rome etc and also they called their another friend Mohan to take part of this world tour.\n",
    "They started their journey from Hyderabad and spent next 3 months travelling all the wonderful cities in the world and cherish a happy moments!\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Raju, Paris, London, Dubai, Rome, Mohan, Hyderabad]\n"
     ]
    }
   ],
   "source": [
    "propn=[]\n",
    "for token in text:\n",
    "    if token.pos_=='PROPN':\n",
    "        propn.append(token)\n",
    "print(propn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tesla, Walmart, Amazon, Microsoft, Google, Infosys, Reliance, HDFC Bank, Hindustan Unilever, Bharti] 10\n"
     ]
    }
   ],
   "source": [
    "text2 = nlp('''The Top 5 companies in USA are Tesla, Walmart, Amazon, Microsoft, Google and the top 5 companies in \n",
    "India are Infosys, Reliance, HDFC Bank, Hindustan Unilever and Bharti Airtel''')\n",
    "\n",
    "all_companies=[]\n",
    "\n",
    "for ent in text2.ents:\n",
    "    if ent.label_=='ORG':\n",
    "        all_companies.append(ent)\n",
    "\n",
    "print(all_companies,len(all_companies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming in NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer=PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"eating\", \"eats\", \"eat\", \"ate\", \"adjustable\", \"rafting\", \"ability\", \"meeting\"]\n",
    "\n",
    "for word in words:\n",
    "    print(word,'|',stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization in spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mando | Mando\n",
      "talked | talk\n",
      "for | for\n",
      "3 | 3\n",
      "hours | hour\n",
      "although | although\n",
      "talking | talk\n",
      "is | be\n",
      "n't | not\n",
      "his | his\n",
      "thing | thing\n"
     ]
    }
   ],
   "source": [
    "#import spacy \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"Mando talked for 3 hours although talking isn't his thing\")\n",
    "doc1 = nlp(\"eating eats eat ate adjustable rafting ability meeting better\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token,'|',token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parts of Speech Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon | PROPN | proper noun\n",
      "flew | VERB | verb\n",
      "to | ADP | adposition\n",
      "mars | NOUN | noun\n",
      "yesterday | NOUN | noun\n",
      ". | PUNCT | punctuation\n",
      "He | PRON | pronoun\n",
      "carried | VERB | verb\n",
      "biryani | ADJ | adjective\n",
      "masala | NOUN | noun\n",
      "with | ADP | adposition\n",
      "him | PRON | pronoun\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Elon flew to mars yesterday. He carried biryani masala with him\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token,'|',token.pos_,'|',spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow | INTJ | interjection | UH | interjection\n",
      "! | PUNCT | punctuation | . | punctuation mark, sentence closer\n",
      "Dr. | PROPN | proper noun | NNP | noun, proper singular\n",
      "Strange | PROPN | proper noun | NNP | noun, proper singular\n",
      "made | VERB | verb | VBD | verb, past tense\n",
      "265 | NUM | numeral | CD | cardinal number\n",
      "million | NUM | numeral | CD | cardinal number\n",
      "$ | NUM | numeral | CD | cardinal number\n",
      "on | ADP | adposition | IN | conjunction, subordinating or preposition\n",
      "the | DET | determiner | DT | determiner\n",
      "very | ADV | adverb | RB | adverb\n",
      "first | ADJ | adjective | JJ | adjective (English), other noun-modifier (Chinese)\n",
      "day | NOUN | noun | NN | noun, singular or mass\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Wow! Dr. Strange made 265 million $ on the very first day\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token,'|',token.pos_,'|',spacy.explain(token.pos_),'|',token.tag_,'|',spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "earnings_text=\"\"\"Microsoft Corp. today announced the following results for the quarter ended December 31, 2021, as compared to the corresponding period of last fiscal year:\n",
    "\n",
    "·         Revenue was $51.7 billion and increased 20%\n",
    "·         Operating income was $22.2 billion and increased 24%\n",
    "·         Net income was $18.8 billion and increased 21%\n",
    "·         Diluted earnings per share was $2.48 and increased 22%\n",
    "“Digital technology is the most malleable resource at the world’s disposal to overcome constraints and reimagine everyday work and life,” said Satya Nadella, chairman and chief executive officer of Microsoft. “As tech as a percentage of global GDP continues to increase, we are innovating and investing across diverse and growing markets, with a common underlying technology stack and an operating model that reinforces a common strategy, culture, and sense of purpose.”\n",
    "“Solid commercial execution, represented by strong bookings growth driven by long-term Azure commitments, increased Microsoft Cloud revenue to $22.1 billion, up 32% year over year” said Amy Hood, executive vice president and chief financial officer of Microsoft.\"\"\"\n",
    "\n",
    "doc=nlp(earnings_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{96: 13,\n",
       " 92: 46,\n",
       " 100: 24,\n",
       " 90: 9,\n",
       " 85: 16,\n",
       " 93: 16,\n",
       " 97: 27,\n",
       " 98: 1,\n",
       " 84: 20,\n",
       " 103: 10,\n",
       " 87: 6,\n",
       " 99: 5,\n",
       " 89: 12,\n",
       " 86: 3,\n",
       " 94: 3,\n",
       " 95: 2}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = doc.count_by(spacy.attrs.POS)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROPN | 13\n",
      "NOUN | 46\n",
      "VERB | 24\n",
      "DET | 9\n",
      "ADP | 16\n",
      "NUM | 16\n",
      "PUNCT | 27\n",
      "SCONJ | 1\n",
      "ADJ | 20\n",
      "SPACE | 10\n",
      "AUX | 6\n",
      "SYM | 5\n",
      "CCONJ | 12\n",
      "ADV | 3\n",
      "PART | 3\n",
      "PRON | 2\n"
     ]
    }
   ],
   "source": [
    "for k,v in count.items():\n",
    "    print(doc.vocab[k].text,'|',v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc | ORG | Companies, agencies, institutions, etc.\n",
      "$45 billion | MONEY | Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,'|',ent.label_,'|',spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is going to acquire twitter for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $45 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CARDINAL',\n",
       " 'DATE',\n",
       " 'EVENT',\n",
       " 'FAC',\n",
       " 'GPE',\n",
       " 'LANGUAGE',\n",
       " 'LAW',\n",
       " 'LOC',\n",
       " 'MONEY',\n",
       " 'NORP',\n",
       " 'ORDINAL',\n",
       " 'ORG',\n",
       " 'PERCENT',\n",
       " 'PERSON',\n",
       " 'PRODUCT',\n",
       " 'QUANTITY',\n",
       " 'TIME',\n",
       " 'WORK_OF_ART']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_labels['ner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jamsetji | GPE | Countries, cities, states\n",
      "Tata | ORG | Companies, agencies, institutions, etc.\n",
      "1968 | DATE | Absolute or relative dates or periods\n"
     ]
    }
   ],
   "source": [
    "doc= nlp('Jamsetji tata founded Tata in 1968')\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,'|',ent.label_,'|',spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "s1=Span(doc, 0 ,2 ,label='PER')\n",
    "\n",
    "doc.set_ents([s1],default='unmodified')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jamsetji tata | PER | Named person or family.\n",
      "Tata | ORG | Companies, agencies, institutions, etc.\n",
      "1968 | DATE | Absolute or relative dates or periods\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text,'|',ent.label_,'|',spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla | ORG | Companies, agencies, institutions, etc.\n",
      "Twitter | PRODUCT | Objects, vehicles, foods, etc. (not services)\n",
      "$45 billion | MONEY | Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "doc= nlp('Tesla is going to acquire Twitter for $45 billion')\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,'|',ent.label_,'|',spacy.explain(ent.label_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
